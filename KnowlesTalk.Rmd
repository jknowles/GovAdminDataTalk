---
title: 'Using Administrative Data to Conduct Rigorous Evaluations: Promises and Challenges'
author: "Jared Knowles"
date: "Monday, October 20, 2014"
output:
  ioslides_presentation:
    css: custom.css
    smaller: yes
---

## Introduction

- Why does causality matter for policy making?
- Why do we like experiments?
- Why do we hate experiments?
- Turning a weakness into a strength
- What does policy analysis look like in the real world?


## Causality Matters

![correlation does not equal causation](assets/PiratesVsTemp.png)

- But politics has a way of trying to make it seem so

## But it's not everything

- Decisions have to be made with best evidence available
- Ruling out alternative explanations generally comes with increased costs
- Scientific process downplays the subjective judgments of domain experts in 
favor of more expensive objective measurement
- May 

## Experiments, Yay!

![explosions are fun](assets/menthoscoke.jpg)

## Experiments Do

- Provide solid internal reliability when done well
- Provide idealized estimates of impact
- Often test interventions in a best-case scenario
- Provide clean data with transparent analyses to draw conclusions

## Experiments, Boo!

![until they are not](assets/beakerbunsen.jpg)

## Experiments Do Not

- Have external reliability. Does the lab resemble the real world?
- Often ignore the importance of precise implementation and often fail to 
measure the variability in implementation that will occur. 
- Do not help understand the reasons program attrition might occur
- Are limited in their ability to test the variability in response to treatment 
under different conditions -- recruiting and retaining subjects is \$\$

## Experiments in Policy

- A randomized controlled trial (RCT) in policy is rarely possible and rarely 
desirable
- RCTs require a level of control and authority that regulators rarely 
possess
- RCTs require a level of monitoring and tracking that may not be appropriate 
for a governmental entity
- Recruiting and retaining subjects is expensive and can substantially add to the 
bill of a policy
- Lack of external validity makes it hard to say "Program X worked in the trial, 
let's go to scale."

## An Aside on the Policy Process

## John Conway's Game of Life

> - Simple rules in large systems create emergent properties that are complex 
and unpredictable
> - A simple example is John Conway's Game of Life, played on a two-dimensional 
grid
> - Any live cell with fewer than two neighbors dies
> - Any live cell with two or three neighbors lives
> - Any live cell with more than three live neighbors dies
> - Any dead cell with exactly three live neighbors becomes a live cell
> - These simple patterns result in emergent patterns that stabilize in 
unpredictable but orderly ways

```{r, echo=FALSE, results='hide', message=FALSE, eval=FALSE}
library(caTools)
# The game.of.life() function ------------------
# Arguments:
# side - side of the game of life arena (matrix)
# steps - number of animation steps
# filename - name of the animated gif file
 
game.of.life <- function(side, steps, filename){
   
  # the sideXside matrix, filled up with binomially
  # distributed individuals
  X <- matrix(nrow=side, ncol=side)
  X[] <- rbinom(side^2,1,0.4)
   
  # array that stores all of the simulation steps
  # (so that it can be exported as a gif)
  storage <- array(0, c(side, side, steps))
 
  # the simulation                                             
  for (i in 1:steps)
  {
     # make the shifted copies of the original array
     allW = cbind( rep(0,side) , X[,-side] )
     allNW = rbind(rep(0,side),cbind(rep(0,side-1),X[-side,-side]))
     allN = rbind(rep(0,side),X[-side,])
     allNE = rbind(rep(0,side),cbind(X[-side,-1],rep(0,side-1)))
     allE = cbind(X[,-1],rep(0,side))
     allSE = rbind(cbind(X[-1,-1],rep(0,side-1)),rep(0,side))
     allS = rbind(X[-1,],rep(0,side))
     allSW = rbind(cbind(rep(0,side-1),X[-1,-side]),rep(0,side))
      
     # summation of the matrices
     X2 <- allW + allNW + allN + allNE + allE + allSE + allS + allSW
      
     # the rules of GoL are applied using logical subscripting
     X3 <- X
     X3[X==0 & X2==3] <- 1
     X3[X==1 & X2<2] <- 0
     X3[X==1 & X2>3] <- 0
     X <- X3
      
     # each simulation step is stored
     storage[,,i] <- X2
     # note that I am storing the array of Ni values -
     # - this is in order to make the animation prettier
   }
    
   storage <- storage/max(storage) # scaling the results
                                   # to a 0-1 scale
 
   # writing the results into an animated gif
   write.gif(storage, filename, col="jet", delay=5)
}
 
game.of.life(side=450, steps=300, file="assets/conway.gif")

# http://www.r-bloggers.com/fast-conways-game-of-life-in-r/ 
gc()
```

## Results

![Conway's Game of Life](assets/conway.gif)

## Policy is Not Unlike This

- Start with simple rules
- Add more simple rules
- Regulated entities react to these rules
- Observe emergent properties
- Repeat

## A Thought Experiment

- Consider education policy for a moment
- An experiment done in 10 school districts statewide shows promising test score 
improvements for 5th grade students in reading with a sample of almost 4,000 students
- The intervention is a 6 week reading program focused on badgers and balloons
- The program randomly assigned students to receive the treatment, or to receive 
their regular classroom instruction

## What are some threats to external validity here?

> - What is the control group?
> - How were the schools selected to participate?


## Challenge

Even if we get the experiment perfect, the evidence it generates is not sufficient 
to guarantee success at scale. 

Non-experimental methods impose a lower cost and lower risk, while providing 
evidence useful in discussions about how to scale. 

## How to spot the opportunity?

- Pilots and phase-ins
- Budget cuts and phase-outs
- Policy resistance
- Policy shifts due to elections (changing priorities)
- Philanthropy
- Local innovation
- Natural experiments (boundary changes, closures, etc.)

## Design vs. Serendipity

- Sometimes we have the privilege of designing an quasi-experimental evaluation 
into a public policy
- Most of the time we have the joy of trying to find a quasi-experimental hook 
to retrospectively evaluate an existing program
- Designed evaluation is preferable, but the sell can be hard
- Retrospective evaluation may not always be possible or only the least rigorous 
methods may apply


## Why? Linked Administrative Systems

![Punch Card Stack](assets/punchcardstack.jpg)

## Why? Longitudinal Records

- If we record everything, we can use instant reply to evaluate policy changes 
retrospectively

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(eeptools)
x <- seq(as.Date("2000/1/1"), as.Date("2016/1/1"), "months")
x1 <- seq(1:length(x))
x1[x1 > 48 & x1 < 170] <- x1[x1 > 52 & x1 < 120] + rnorm(length(x1[x1 > 52 & x1 < 120]), -30, 1)
x1 <- log(x1)
y <- 100 + rnorm(length(x1), 5, 0.2) * x1 + rnorm(length(x1), 1, 2)

qplot(x, y) + theme_dpi() + geom_smooth() + geom_line(alpha = 0.4) + 
  geom_vline(xintercept = as.numeric(as.Date("2004-04-01"))) + 
  geom_vline(xintercept = as.numeric(as.Date("2013-04-01")))+
  labs(x = "Year", y = "Scores", 
       title = "Example Longitudinal Series of Ratings")


```

## Why? Big Data

![Say big data](assets/say-big-data-one-more-time.jpg)

## Summary: Strength of Administrative Records

- Big sample size
- Good chance for pre- and post-tests


## Limitations: Measures

We are stuck with the measurements we already have which may be outdated or 
a poor fit.

![bad scales](assets/oldtimeyscale.jpg)

## Limitations: Unobservables and Selection Bias


## Limitations: Limited Ability to Understand the Mechanisms At Work

- Cannot often go back and figure out why program happened
- Little implementation data, if any

## Quasi-Experiment Examples in the Wild

- Regression discontinuity through ELL reclassification
- Fixed effects regression through BLBC program offer
- Lottery for pre-college scholarship programs
- Differences in differences evaluation of community learning centers (CLCs)

## ELL Reclassification

- Most states move English language learning students out of ELL services after 
they score a certain score on a standardized test
- The cutpoint in the test score is arbitrary (though test is sensitive around 
this range)
- Students who just miss are identical to students who barely pass
- Use this "randomization" to evaluate the effect of leaving ELL services
- Leaving ELL services produces negative effect
- Cannot generalize this to a broad population, very narrow policy question, 
tied to specific test threshold


## Wisconsin Bilingual Bicultural Program

- Look at a number of models of program effectiveness using student records
- Look at performance of students who switch in and out of BLBC due to program 
start and stop 
- Look at performance of students who move between schools with and without 
BLBC programs


## Pre-college Scholarship Lottery

- Compare outcomes of students who were denied access to pre-college scholarship 
program due to oversubscription to students who attend program
- Able to compare years in program as students reapply
- Need to be sure access is random -- employ checks for randomness in denials

## DD in CLC



## Caveats

- Large datasets *can* work against you by giving false precision to estimates due to large sample size
- Large datasets *can* make robust techniques of estimating errors trickier such as 
Bayesian estimation, simulation, and model visualization
- All the caveats about selection on unobservables apply. Content expertise will 
help you evaluate how credible of a validity threat this provides.


## Come work for DPI

- DPI is hiring a research analyst part time from Novemeber/December through 
the spring
- Flexible hours (10-30 hours a week, depending on availability)
- Work on a variety of data analyses in support of DPI budget, legislative, and 
administrative initiatives
- To apply, send me a short resume and cover letter [jared.knowles@dpi.wi.gov](mailto:jared.knowles@dpi.wi.gov)





